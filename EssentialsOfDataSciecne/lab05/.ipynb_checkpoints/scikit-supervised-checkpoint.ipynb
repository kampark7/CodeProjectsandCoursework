{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'images', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "data = datasets.fetch_olivetti_faces()\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.. _olivetti_faces_dataset:\\n\\nThe Olivetti faces dataset\\n--------------------------\\n\\n`This dataset contains a set of face images`_ taken between April 1992 and \\nApril 1994 at AT&T Laboratories Cambridge. The\\n:func:`sklearn.datasets.fetch_olivetti_faces` function is the data\\nfetching / caching function that downloads the data\\narchive from AT&T.\\n\\n.. _This dataset contains a set of face images: http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html\\n\\nAs described on the original website:\\n\\n    There are ten different images of each of 40 distinct subjects. For some\\n    subjects, the images were taken at different times, varying the lighting,\\n    facial expressions (open / closed eyes, smiling / not smiling) and facial\\n    details (glasses / no glasses). All the images were taken against a dark\\n    homogeneous background with the subjects in an upright, frontal position \\n    (with tolerance for some side movement).\\n\\n**Data Set Characteristics:**\\n\\n    =================   =====================\\n    Classes                                40\\n    Samples total                         400\\n    Dimensionality                       4096\\n    Features            real, between 0 and 1\\n    =================   =====================\\n\\nThe image is quantized to 256 grey levels and stored as unsigned 8-bit \\nintegers; the loader will convert these to floating point values on the \\ninterval [0, 1], which are easier to work with for many algorithms.\\n\\nThe \"target\" for this database is an integer from 0 to 39 indicating the\\nidentity of the person pictured; however, with only 10 examples per class, this\\nrelatively small dataset is more interesting from an unsupervised or\\nsemi-supervised perspective.\\n\\nThe original dataset consisted of 92 x 112, while the version available here\\nconsists of 64x64 images.\\n\\nWhen using these images, please give credit to AT&T Laboratories Cambridge.\\n'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['DESCR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.data\n",
    "Y = data.target\n",
    "df = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y'] = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4087</th>\n",
       "      <th>4088</th>\n",
       "      <th>4089</th>\n",
       "      <th>4090</th>\n",
       "      <th>4091</th>\n",
       "      <th>4092</th>\n",
       "      <th>4093</th>\n",
       "      <th>4094</th>\n",
       "      <th>4095</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.309917</td>\n",
       "      <td>0.367769</td>\n",
       "      <td>0.417355</td>\n",
       "      <td>0.442149</td>\n",
       "      <td>0.528926</td>\n",
       "      <td>0.607438</td>\n",
       "      <td>0.657025</td>\n",
       "      <td>0.677686</td>\n",
       "      <td>0.690083</td>\n",
       "      <td>0.685950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.669421</td>\n",
       "      <td>0.652893</td>\n",
       "      <td>0.661157</td>\n",
       "      <td>0.475207</td>\n",
       "      <td>0.132231</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.161157</td>\n",
       "      <td>0.157025</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.471074</td>\n",
       "      <td>0.512397</td>\n",
       "      <td>0.557851</td>\n",
       "      <td>0.595041</td>\n",
       "      <td>0.640496</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.702479</td>\n",
       "      <td>0.710744</td>\n",
       "      <td>0.702479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157025</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.400826</td>\n",
       "      <td>0.491736</td>\n",
       "      <td>0.528926</td>\n",
       "      <td>0.586777</td>\n",
       "      <td>0.657025</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.685950</td>\n",
       "      <td>0.702479</td>\n",
       "      <td>0.698347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132231</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.128099</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.144628</td>\n",
       "      <td>0.140496</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.198347</td>\n",
       "      <td>0.194215</td>\n",
       "      <td>0.194215</td>\n",
       "      <td>0.194215</td>\n",
       "      <td>0.190083</td>\n",
       "      <td>0.190083</td>\n",
       "      <td>0.243802</td>\n",
       "      <td>0.404959</td>\n",
       "      <td>0.483471</td>\n",
       "      <td>0.516529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.657025</td>\n",
       "      <td>0.685950</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.743802</td>\n",
       "      <td>0.764463</td>\n",
       "      <td>0.752066</td>\n",
       "      <td>0.752066</td>\n",
       "      <td>0.739669</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.582645</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0.648760</td>\n",
       "      <td>0.690083</td>\n",
       "      <td>0.694215</td>\n",
       "      <td>0.714876</td>\n",
       "      <td>0.723140</td>\n",
       "      <td>0.731405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161157</td>\n",
       "      <td>0.177686</td>\n",
       "      <td>0.173554</td>\n",
       "      <td>0.177686</td>\n",
       "      <td>0.177686</td>\n",
       "      <td>0.177686</td>\n",
       "      <td>0.177686</td>\n",
       "      <td>0.173554</td>\n",
       "      <td>0.173554</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395</td>\n",
       "      <td>0.400826</td>\n",
       "      <td>0.495868</td>\n",
       "      <td>0.570248</td>\n",
       "      <td>0.632231</td>\n",
       "      <td>0.648760</td>\n",
       "      <td>0.640496</td>\n",
       "      <td>0.661157</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.665289</td>\n",
       "      <td>0.698347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.396694</td>\n",
       "      <td>0.264463</td>\n",
       "      <td>0.099174</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.243802</td>\n",
       "      <td>0.247934</td>\n",
       "      <td>0.161157</td>\n",
       "      <td>0.157025</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>396</td>\n",
       "      <td>0.367769</td>\n",
       "      <td>0.367769</td>\n",
       "      <td>0.351240</td>\n",
       "      <td>0.301653</td>\n",
       "      <td>0.247934</td>\n",
       "      <td>0.247934</td>\n",
       "      <td>0.367769</td>\n",
       "      <td>0.512397</td>\n",
       "      <td>0.574380</td>\n",
       "      <td>0.628099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.334711</td>\n",
       "      <td>0.289256</td>\n",
       "      <td>0.285124</td>\n",
       "      <td>0.338843</td>\n",
       "      <td>0.404959</td>\n",
       "      <td>0.458678</td>\n",
       "      <td>0.487603</td>\n",
       "      <td>0.512397</td>\n",
       "      <td>0.549587</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>397</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.533058</td>\n",
       "      <td>0.607438</td>\n",
       "      <td>0.628099</td>\n",
       "      <td>0.657025</td>\n",
       "      <td>0.632231</td>\n",
       "      <td>0.657025</td>\n",
       "      <td>0.669421</td>\n",
       "      <td>0.673554</td>\n",
       "      <td>0.702479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.161157</td>\n",
       "      <td>0.161157</td>\n",
       "      <td>0.173554</td>\n",
       "      <td>0.157025</td>\n",
       "      <td>0.177686</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.190083</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>398</td>\n",
       "      <td>0.214876</td>\n",
       "      <td>0.219008</td>\n",
       "      <td>0.219008</td>\n",
       "      <td>0.223140</td>\n",
       "      <td>0.210744</td>\n",
       "      <td>0.202479</td>\n",
       "      <td>0.276859</td>\n",
       "      <td>0.400826</td>\n",
       "      <td>0.487603</td>\n",
       "      <td>0.549587</td>\n",
       "      <td>...</td>\n",
       "      <td>0.392562</td>\n",
       "      <td>0.367769</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.479339</td>\n",
       "      <td>0.524793</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.574380</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.603306</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>399</td>\n",
       "      <td>0.516529</td>\n",
       "      <td>0.462810</td>\n",
       "      <td>0.280992</td>\n",
       "      <td>0.252066</td>\n",
       "      <td>0.247934</td>\n",
       "      <td>0.367769</td>\n",
       "      <td>0.574380</td>\n",
       "      <td>0.615702</td>\n",
       "      <td>0.661157</td>\n",
       "      <td>0.615702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.264463</td>\n",
       "      <td>0.293388</td>\n",
       "      <td>0.301653</td>\n",
       "      <td>0.293388</td>\n",
       "      <td>0.322314</td>\n",
       "      <td>0.322314</td>\n",
       "      <td>0.359504</td>\n",
       "      <td>0.355372</td>\n",
       "      <td>0.384298</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 4097 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.309917  0.367769  0.417355  0.442149  0.528926  0.607438  0.657025   \n",
       "1    0.454545  0.471074  0.512397  0.557851  0.595041  0.640496  0.681818   \n",
       "2    0.318182  0.400826  0.491736  0.528926  0.586777  0.657025  0.681818   \n",
       "3    0.198347  0.194215  0.194215  0.194215  0.190083  0.190083  0.243802   \n",
       "4    0.500000  0.545455  0.582645  0.623967  0.648760  0.690083  0.694215   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "395  0.400826  0.495868  0.570248  0.632231  0.648760  0.640496  0.661157   \n",
       "396  0.367769  0.367769  0.351240  0.301653  0.247934  0.247934  0.367769   \n",
       "397  0.500000  0.533058  0.607438  0.628099  0.657025  0.632231  0.657025   \n",
       "398  0.214876  0.219008  0.219008  0.223140  0.210744  0.202479  0.276859   \n",
       "399  0.516529  0.462810  0.280992  0.252066  0.247934  0.367769  0.574380   \n",
       "\n",
       "            7         8         9  ...      4087      4088      4089  \\\n",
       "0    0.677686  0.690083  0.685950  ...  0.669421  0.652893  0.661157   \n",
       "1    0.702479  0.710744  0.702479  ...  0.157025  0.136364  0.148760   \n",
       "2    0.685950  0.702479  0.698347  ...  0.132231  0.181818  0.136364   \n",
       "3    0.404959  0.483471  0.516529  ...  0.636364  0.657025  0.685950   \n",
       "4    0.714876  0.723140  0.731405  ...  0.161157  0.177686  0.173554   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "395  0.636364  0.665289  0.698347  ...  0.396694  0.264463  0.099174   \n",
       "396  0.512397  0.574380  0.628099  ...  0.334711  0.289256  0.285124   \n",
       "397  0.669421  0.673554  0.702479  ...  0.148760  0.152893  0.161157   \n",
       "398  0.400826  0.487603  0.549587  ...  0.392562  0.367769  0.409091   \n",
       "399  0.615702  0.661157  0.615702  ...  0.264463  0.293388  0.301653   \n",
       "\n",
       "         4090      4091      4092      4093      4094      4095   y  \n",
       "0    0.475207  0.132231  0.148760  0.152893  0.161157  0.157025   0  \n",
       "1    0.152893  0.152893  0.152893  0.152893  0.152893  0.152893   0  \n",
       "2    0.128099  0.148760  0.144628  0.140496  0.148760  0.152893   0  \n",
       "3    0.727273  0.743802  0.764463  0.752066  0.752066  0.739669   0  \n",
       "4    0.177686  0.177686  0.177686  0.177686  0.173554  0.173554   0  \n",
       "..        ...       ...       ...       ...       ...       ...  ..  \n",
       "395  0.181818  0.243802  0.247934  0.161157  0.157025  0.136364  39  \n",
       "396  0.338843  0.404959  0.458678  0.487603  0.512397  0.549587  39  \n",
       "397  0.161157  0.173554  0.157025  0.177686  0.148760  0.190083  39  \n",
       "398  0.479339  0.524793  0.545455  0.574380  0.590909  0.603306  39  \n",
       "399  0.293388  0.322314  0.322314  0.359504  0.355372  0.384298  39  \n",
       "\n",
       "[400 rows x 4097 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.values\n",
    "Y = train.y.values\n",
    "\n",
    "X_test = test.values\n",
    "y_test = test.y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = linear_model.LogisticRegression(penalty='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.975"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(guess, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.825"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = linear_model.SGDClassifier(penalty = 'l2')\n",
    "clf.fit(X,Y)\n",
    "guess_2 = clf.predict(X_test)\n",
    "accuracy_score(guess_2, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n",
    "model.fit(X,Y)\n",
    "guess_3 = model.predict(X_test)\n",
    "accuracy_score(guess_3, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the features?**\n",
    "The features are the different faces of people and the 10 different ways the face was manipulated\n",
    "\n",
    "**What was it your classifier was trying to classify? (Hint: You'll need to read the description of the data--this constitutes the classification \"task\"; i.e., the mapping from features in X to the classification labels in y)**\n",
    "\n",
    "The classifier was trying to classify the realness of the face of a given person (i.e. the \"Target\"). So the classifier was taking the various inputs from the data and checking if they were acurattly identified.\n",
    "\n",
    "**What is the baseline for this dataset? (Hint: what is the most common type that could be classified?)** The baseline is even for each type of face in the data (0.1) for each type. This would then add up to 100%\n",
    "\n",
    "\n",
    "**How well did your three classifiers do above the baseline.**\n",
    "Each classifier did fairly well they all were at least .8 or above which I think would be good. However, since the data set is smaller it could be considered not a good classification done by the classifier.\n",
    "\n",
    "**Read a little bit about each classifier. Why do you think one classifier did better than another classifier?**\n",
    "I think that the linear regression did the best out of all of them because the data has distinct classifications that do not differ in size and their features are always between 0 and 1. The SGD classifier is best for gradient learning and have a 0 mean which this data does not. The random forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open-Ended Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Models are Unsupervised Multitask Learners\n",
    "https://paperswithcode.com/paper/language-models-are-unsupervised-multitask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the input features?**\n",
    "The input features of this data set were single domains of text (i.e. newpaper articles or fiction books). They data was supposed to be as large and complex as possinle to collect natural language demonstrationsof tasks. The target dataset was the Winograd Schema Challenge. There were specifically four different language models.\n",
    "\n",
    "**What are the class labels?**\n",
    "There were different ways they tested the language detection so there were different labels for each. One specific example was using a Children's Book Test. The labels were named entities, nouns, verbs, and prepositions. \n",
    "\n",
    "**How is the data represented?**\n",
    "The data is represented as a general language model which should be able to compute the probibility of any string. This specfiic data set used Byte Pair Encoding whish is a type of encoding that can help bypass or merge character categories.\n",
    "\n",
    "**Do you think any of the three classification approaches you used for the Olivettie Faces above would work for this task?** I think that potentially the Logistic Regression would work for this task becasuse this task is testing accuracy and it seems to be on a linear scale of \"did detect this label\" or \"did not detect this label\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Latent Dynamics for Planning from Pixels\n",
    "https://paperswithcode.com/paper/learning-latent-dynamics-for-planning-from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the input features?**\n",
    "The input features were Action repeat, seed episodes, collect interval, batch size, chunk lenght and learning rate.\n",
    "\n",
    "**What are the class labels?**\n",
    "I believe the class labels for these features are standard variational bound, observation overshooting and latent overshooting. The different arrows in the papers figures represent different types of losses in the system.\n",
    "\n",
    "**How is the data represented?**\n",
    "The data is represented as seed episodes that are trained and they add one additional episode to the data set every collection interval. There are 6 different types of data being studied the carpole swing up, reacher easy, cheetah run, finger spin, cup catch and walker walk. The plots in the paper show the overall test perfromances over the number of collected seed episodes.\n",
    "\n",
    "**Do you think any of the three classification approaches you used for the Olivettie Faces above would work for this task?**\n",
    "I am not sure that any of the classifiers I used would work for this data set becasue there is a large amount of things this data set is testing for that I don't think one of these other simple classifiers would be able to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
