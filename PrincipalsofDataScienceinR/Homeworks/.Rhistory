add_model(rf_model)
#Make the fit to the random forest with our workflow formula and training data
rf_fit <- fit(rf_wf, rf_train)
rf_fit %>%
predict(rf_test) %>%
bind_cols(rf_test) %>%
metrics(truth = species, estimate = .pred_class) #accuracy of the model
(penguin_boots <- bootstraps(df_train, times = 250))
boot_outcomes <- rf_wf %>%
fit_resamples(resamples = penguin_boots,
control = control_resamples(save_pred = TRUE),
#You can specify which metrics you want to calculate
metrics = metric_set(roc_auc, accuracy, kap))
#This dataframe has information on each model, trained and tested on each bootstrap sample.
#We can use the "collect metrics" function to get an aggregate measure of model accuracy and roc-auc across all bootstraps and trained models.
boot_outcomes %>%
collect_metrics()
library(tidyverse)
library(tidymodels)
library(palmerpenguins)
df <- penguins %>% filter(complete.cases(.))
df_split <- initial_split(df)
df_train <- training(df_split)
df_test <- testing(df_split)
df
library(rpart)
library(rpart.plot)
dec_tree_model <- decision_tree(tree_depth = 5) %>% #setting tree depth after tuning tree
set_engine("rpart") %>%
set_mode("classification")
dec_tree_fit <- workflow() %>%
add_formula(species ~ island + bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g + body_mass_g + sex) %>% #choosing a good amount of variables to have the tree split with
add_model(dec_tree_model) %>%
fit(df_train) %>%
pull_workflow_fit()
rpart.plot(dec_tree_fit$fit)
dec_tree_fit %>%
predict(df_test) %>%
bind_cols(df_test) %>%
metrics(truth = species, estimate = .pred_class)
dec_tree_fit %>%
predict(df_test) %>% #predicts using test data
bind_cols(df_test) %>% #Binding those columns to the decsiion tree fit
conf_mat(truth = species, estimate = .pred_class) #Mkaes a confusion matrix as Work as the truth from our prediction
precision = (36 + 12 + 33) / (36 + 12 + 33 + 3)
precision
library(tidyverse)
library(tidymodels)
library(palmerpenguins)
df <- penguins %>% filter(complete.cases(.))
df_split <- initial_split(df)
df_train <- training(df_split)
df_test <- testing(df_split)
df
library(rpart)
library(rpart.plot)
dec_tree_model <- decision_tree(tree_depth = 5) %>% #setting tree depth after tuning tree
set_engine("rpart") %>%
set_mode("classification")
dec_tree_fit <- workflow() %>%
add_formula(species ~ island + bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g + body_mass_g + sex) %>% #choosing a good amount of variables to have the tree split with
add_model(dec_tree_model) %>%
fit(df_train) %>%
pull_workflow_fit()
rpart.plot(dec_tree_fit$fit)
dec_tree_fit %>%
predict(df_test) %>%
bind_cols(df_test) %>%
metrics(truth = species, estimate = .pred_class)
dec_tree_fit %>%
predict(df_test) %>% #predicts using test data
bind_cols(df_test) %>% #Binding those columns to the decsiion tree fit
conf_mat(truth = species, estimate = .pred_class) #Mkaes a confusion matrix as Work as the truth from our prediction
precision = (36 + 12 + 33) / (36 + 12 + 33 + 3)
precision
dec_tree_fit %>%
predict(df_test) %>%
bind_cols(df_test) %>%
metrics(truth = species, estimate = .pred_class)
dec_tree_fit %>%
predict(df_test) %>% #predicts using test data
bind_cols(df_test) %>% #Binding those columns to the decsiion tree fit
conf_mat(truth = species, estimate = .pred_class) #Mkaes a confusion matrix as Work as the truth from our prediction
precision = (31 + 14 + 30) / (31 + 14 + 30 + 2)
precision
library(randomForest)
library(vip)
rf_model <- rand_forest(trees = 100, mtry = 4) %>% #using 150 trees with 15 variables per tree
set_mode("classification") %>%
set_engine("randomForest", importance = TRUE)
#having to edit our train and test sets to not contain NA values
rf_train <- df_train %>%
select(c(species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, body_mass_g, sex)) %>% #Be specific about variables needing to be used
filter(complete.cases(.))
rf_test <- df_test %>%
select(c(species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, body_mass_g, sex)) %>%
filter(complete.cases(.))
#create a workflow formula to include in our model
rf_wf <- workflow() %>%
add_formula(species ~ island + bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g + body_mass_g + sex) %>%
add_model(rf_model)
#Make the fit to the random forest with our workflow formula and training data
rf_fit <- fit(rf_wf, rf_train)
rf_fit %>%
predict(rf_test) %>%
bind_cols(rf_test) %>%
metrics(truth = species, estimate = .pred_class) #accuracy of the model
(penguin_boots <- bootstraps(df_train, times = 250))
boot_outcomes <- rf_wf %>%
fit_resamples(resamples = penguin_boots,
control = control_resamples(save_pred = TRUE),
#You can specify which metrics you want to calculate
metrics = metric_set(roc_auc, accuracy, kap))
#This dataframe has information on each model, trained and tested on each bootstrap sample.
#We can use the "collect metrics" function to get an aggregate measure of model accuracy and roc-auc across all bootstraps and trained models.
boot_outcomes %>%
collect_metrics()
#Another way to do it, to get data for each bootstrap:
boot_outcomes %>%
unnest(.metrics)
#Also, a confidence matrix
boot_outcomes %>%
conf_mat_resampled()
# Finally, we use our model on our test data (which we did NOT include in our bootstrap pool)
rf_wf %>%
fit(df_test) %>%
predict(df_test) %>%
bind_cols(df_test) %>%
metrics(estimate = .pred_class, truth = species)
(penguin_folds <- vfold_cv(df_train, v= 10, repeats = 1))
rf_tune <- workflow() %>%
add_model(rf_reg) %>%
add_formula(species ~ island + bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g + body_mass_g + sex)
rf_tune <- workflow() %>%
add_model(rf_model) %>%
add_formula(species ~ island + bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g + body_mass_g + sex)
tuned_rf <- tune_grid(rf_tune, resamples = peng_folds, grid = 36, metrics = metric_set(rmse, mae, rsq))
rf_tune <- workflow() %>%
add_model(rf_model) %>%
add_formula(species ~ island + bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g + body_mass_g + sex)
tuned_rf <- tune_grid(rf_tune, resamples = penguin_folds, grid = 36, metrics = metric_set(rmse, mae, rsq))
rf_tune <- workflow() %>%
add_model(rf_model) %>%
add_formula(species ~ island + bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g + body_mass_g + sex)
tune_rf <- decision_tree(cost_complexity = tune(),
tree_depth = tune()) %>%
set_engine("rpart") %>%
set_mode("classification")
tree_grid <- grid_regular(cost_complexity(),
tree_depth(),
levels = 5)
#Creating workflow
tree_wf <- workflow() %>%
add_model(tune_rf) %>%
add_formula(species ~ island + bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g + body_mass_g + sex)
#Tuning the workflow formula with resamples, the hyperparameters from tree grid
tree_tune <- tree_wf %>%
tune_grid(rf_tune, resamples = penguin_folds, grid = 36, metrics = metric_set(rmse, mae, rsq))
rf_tune <- workflow() %>%
add_model(rf_model) %>%
add_formula(species ~ island + bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g + body_mass_g + sex)
tune_rf <- decision_tree(cost_complexity = tune(),
tree_depth = tune()) %>%
set_engine("rpart") %>%
set_mode("classification")
tree_grid <- grid_regular(cost_complexity(),
tree_depth(),
levels = 5)
#Creating workflow
tree_wf <- workflow() %>%
add_model(tune_rf) %>%
add_formula(species ~ island + bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g + body_mass_g + sex)
#Tuning the workflow formula with resamples, the hyperparameters from tree grid
tree_tune <- tree_wf %>%
tune_grid(rf_tune, resamples = penguin_folds, grid = 36)
rf_tune <- workflow() %>%
add_model(rf_model) %>%
add_formula(species ~ island + bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g + body_mass_g + sex)
tune_rf <- decision_tree(cost_complexity = tune(),
tree_depth = tune()) %>%
set_engine("rpart") %>%
set_mode("classification")
tree_grid <- grid_regular(cost_complexity(),
tree_depth(),
levels = 5)
#Creating workflow
tree_wf <- workflow() %>%
add_model(tune_rf) %>%
add_formula(species ~ island + bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g + body_mass_g + sex)
#Tuning the workflow formula with resamples, the hyperparameters from tree grid
tree_tune <- tree_wf %>%
tune_grid(resamples = cross_val_folds, grid = tree_grid)
rf_tune <- workflow() %>%
add_model(rf_model) %>%
add_formula(species ~ island + bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g + body_mass_g + sex)
tune_rf <- decision_tree(cost_complexity = tune(),
tree_depth = tune()) %>%
set_engine("rpart") %>%
set_mode("classification")
tree_grid <- grid_regular(cost_complexity(),
tree_depth(),
levels = 5)
#Creating workflow
tree_wf <- workflow() %>%
add_model(tune_rf) %>%
add_formula(species ~ island + bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g + body_mass_g + sex)
#Tuning the workflow formula with resamples, the hyperparameters from tree grid
tree_tune <- tree_wf %>%
tune_grid(resamples = penguin_folds, grid = tree_grid)
#Collecting metrics from our tuned crossfolds.
tree_metrics <- tree_tune %>% collect_metrics()
#Let's identify the top 10 models with the best accuracy, and the best ROC_AUC (closest to 1)
top_5_acc <- tree_metrics %>% filter(.metric == "accuracy") %>% slice_max(mean, n = 5)
top_5_roc_auc <- tree_metrics %>% filter(.metric == "roc_auc") %>% slice_max(mean, n = 5)
acc <- tree_metrics %>%
filter(.metric == "accuracy") %>%
ggplot(aes(x = cost_complexity, y = tree_depth, color = mean)) +
geom_point() +
geom_point(data = top_5_acc, aes(x = cost_complexity, y = tree_depth), color = "red") +
scale_x_continuous(trans="log2") +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
roc_auc <- tree_metrics %>%
filter(.metric == "roc_auc") %>%
ggplot(aes(x = cost_complexity, y = tree_depth, color = mean)) +
geom_point() +
geom_point(data = top_5_roc_auc, aes(x = cost_complexity, y = tree_depth), color = "red") +
scale_x_continuous(trans="log2") +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
library(gridExtra)
grid.arrange(acc, roc_auc)
rf_tune <- workflow() %>%
add_model(rf_model) %>%
add_formula(species ~ island + bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g + body_mass_g + sex)
tune_rf <- decision_tree(cost_complexity = tune(),
tree_depth = tune()) %>%
set_engine("rpart") %>%
set_mode("classification")
tree_grid <- grid_regular(cost_complexity(),
tree_depth(),
levels = 5)
#Creating workflow
tree_wf <- workflow() %>%
add_model(tune_rf) %>%
add_formula(species ~ island + bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g + body_mass_g + sex)
#Tuning the workflow formula with resamples, the hyperparameters from tree grid
tree_tune <- tree_wf %>%
tune_grid(resamples = penguin_folds, grid = tree_grid)
tree_tune
library(tidymodels)
library(tidyverse)
library(palmerpenguins)
library(gridExtra)
rf_spec <- rand_forest() %>%
set_mode("classification") %>%
set_engine("ranger")
rf_wf <- workflow() %>%
add_model(rf_spec) %>%
add_formula(species ~ .)
penguins <- penguins %>% filter(complete.cases(.))
peng_split <- initial_split(penguins)
peng_train <- training(peng_split)
peng_test <- testing(peng_split)
# Training our model on our training data
train_fit <- fit(rf_wf, peng_train)
# Testing our model on our test data
train_fit %>%
predict(peng_test) %>%
bind_cols(peng_test) %>%
metrics(estimate = .pred_class, truth = species)
train_fit %>%
predict(peng_test) %>%
bind_cols(peng_test) %>%
conf_mat(truth = species, estimate = .pred_class)
penguins <- penguins %>% filter(complete.cases(.))
peng_split <- initial_split(penguins)
peng_train <- training(peng_split)
peng_test <- testing(peng_split)
# Training our model on our training data
train_fit <- fit(rf_wf, peng_train)
# Testing our model on our test data
train_fit %>%
predict(peng_test) %>%
bind_cols(peng_test) %>%
metrics(estimate = .pred_class, truth = species)
train_fit %>%
predict(peng_test) %>%
bind_cols(peng_test) %>%
conf_mat(truth = species, estimate = .pred_class)
rf_spec <- rand_forest() %>%
set_mode("classification") %>%
set_engine("ranger")
rf_wf <- workflow() %>%
add_model(rf_spec) %>%
add_formula(species ~ .)
penguins <- penguins %>% filter(complete.cases(.))
peng_split <- initial_split(penguins)
peng_train <- training(peng_split)
peng_test <- testing(peng_split)
# Training our model on our training data
train_fit <- fit(rf_wf, peng_train)
# Testing our model on our test data
train_fit %>%
predict(peng_test) %>%
bind_cols(peng_test) %>%
metrics(estimate = .pred_class, truth = species)
train_fit %>%
predict(peng_test) %>%
bind_cols(peng_test) %>%
conf_mat(truth = species, estimate = .pred_class)
penguins <- penguins %>% filter(complete.cases(.))
peng_split <- initial_split(penguins)
peng_train <- training(peng_split)
peng_test <- testing(peng_split)
# Training our model on our training data
train_fit <- fit(rf_wf, peng_train)
# Testing our model on our test data
train_fit %>%
predict(peng_test) %>%
bind_cols(peng_test) %>%
metrics(estimate = .pred_class, truth = species)
train_fit %>%
predict(peng_test) %>%
bind_cols(peng_test) %>%
conf_mat(truth = species, estimate = .pred_class)
train_fit <- fit(rf_wf, peng_train)
install.packages("ranger")
penguins <- penguins %>% filter(complete.cases(.))
peng_split <- initial_split(penguins)
peng_train <- training(peng_split)
peng_test <- testing(peng_split)
# Training our model on our training data
train_fit <- fit(rf_wf, peng_train)
# Testing our model on our test data
train_fit %>%
predict(peng_test) %>%
bind_cols(peng_test) %>%
metrics(estimate = .pred_class, truth = species)
train_fit %>%
predict(peng_test) %>%
bind_cols(peng_test) %>%
conf_mat(truth = species, estimate = .pred_class)
#Using a function from tidymodels
(penguin_boots <- bootstraps(peng_train, times = 250))
bootstrap_outcomes <- rf_wf %>%
fit_resamples(resamples = penguin_boots,
control = control_resamples(save_pred = TRUE),
#You can specify which metrics you want to calculate
metrics = metric_set(roc_auc, accuracy, kap))
#This dataframe has information on each model, trained and tested on each bootstrap sample.
#We can use the "collect metrics" function to get an aggregate measure of model accuracy and roc-auc across all bootstraps and trained models.
bootstrap_outcomes %>%
collect_metrics()
#Another way to do it, to get data for each bootstrap:
bootstrap_outcomes %>%
unnest(.metrics)
#Also, a confidence matrix
bootstrap_outcomes %>%
conf_mat_resampled()
# Finally, we use our model on our test data (which we did NOT include in our bootstrap pool)
rf_wf %>%
fit(peng_test) %>%
predict(peng_test) %>%
bind_cols(peng_test) %>%
metrics(estimate = .pred_class, truth = species)
#Note again, that we are still taking our cross-folds only from our initial training dataset.
(penguin_folds <- vfold_cv(peng_train, v= 10, repeats = 1))
cross_fold_outcomes <- rf_wf %>%
fit_resamples(penguin_folds, metrics = metric_set(roc_auc, accuracy, kap))
cross_fold_outcomes <- rf_wf %>%
fit_resamples(penguin_folds, metrics = metric_set(roc_auc, accuracy, kap))
#Note again, that we are still taking our cross-folds only from our initial training dataset.
(penguin_folds <- vfold_cv(peng_train, v= 10, repeats = 1))
cross_fold_outcomes <- rf_wf %>%
fit_resamples(penguin_folds, metrics = metric_set(roc_auc, accuracy, kap))
cross_fold_outcomes %>%
collect_metrics()
library(tidymodels)
library(tidyverse)
library(palmerpenguins)
library(gridExtra)
rf_spec <- rand_forest() %>%
set_mode("classification") %>%
set_engine("ranger")
rf_wf <- workflow() %>%
add_model(rf_spec) %>%
add_formula(species ~ .)
penguins <- penguins %>% filter(complete.cases(.))
peng_split <- initial_split(penguins)
peng_train <- training(peng_split)
peng_test <- testing(peng_split)
# Training our model on our training data
train_fit <- fit(rf_wf, peng_train)
# Testing our model on our test data
train_fit %>%
predict(peng_test) %>%
bind_cols(peng_test) %>%
metrics(estimate = .pred_class, truth = species)
train_fit %>%
predict(peng_test) %>%
bind_cols(peng_test) %>%
conf_mat(truth = species, estimate = .pred_class)
#Using a function from tidymodels
(penguin_boots <- bootstraps(peng_train, times = 250))
bootstrap_outcomes <- rf_wf %>%
fit_resamples(resamples = penguin_boots,
control = control_resamples(save_pred = TRUE),
#You can specify which metrics you want to calculate
metrics = metric_set(roc_auc, accuracy, kap))
#This dataframe has information on each model, trained and tested on each bootstrap sample.
#We can use the "collect metrics" function to get an aggregate measure of model accuracy and roc-auc across all bootstraps and trained models.
bootstrap_outcomes %>%
collect_metrics()
#Another way to do it, to get data for each bootstrap:
bootstrap_outcomes %>%
unnest(.metrics)
#Also, a confidence matrix
bootstrap_outcomes %>%
conf_mat_resampled()
# Finally, we use our model on our test data (which we did NOT include in our bootstrap pool)
rf_wf %>%
fit(peng_test) %>%
predict(peng_test) %>%
bind_cols(peng_test) %>%
metrics(estimate = .pred_class, truth = species)
#Note again, that we are still taking our cross-folds only from our initial training dataset.
(penguin_folds <- vfold_cv(peng_train, v= 10, repeats = 1))
cross_fold_outcomes <- rf_wf %>%
fit_resamples(penguin_folds, metrics = metric_set(roc_auc, accuracy, kap))
cross_fold_outcomes %>%
collect_metrics()
rf_reg <- rand_forest(trees = 100, mtry = tune(),min_n = tune()) %>%
set_mode("regression") %>%
set_engine("ranger")
rf_tune <- workflow() %>%
add_model(rf_reg) %>%
add_formula(body_mass_g ~ .)
peng_folds <- vfold_cv(peng_train)
tuned_rf <- tune_grid(rf_tune, resamples = peng_folds, grid = 36, metrics = metric_set(rmse, mae, rsq))
(met_tab <- tuned_rf %>%
#The collect metrics function often knows when to give us aggregated vs individual specs, based on the type of object we give it.
collect_metrics())
mae_top <- met_tab %>% filter(.metric == "mae") %>% slice_min(mean, n=1)
rmse_top <- met_tab %>% filter(.metric == "rmse") %>% slice_min(mean, n=1)
rsq_top <- met_tab %>% filter(.metric == "rsq") %>% slice_max(mean, n=1)
mae_plot <- met_tab %>%
filter(.metric == "mae") %>%
ggplot(aes(x = min_n, y = mtry, color = mean)) +
geom_point() +
geom_point(data = mae_top, aes(x= min_n, y = mtry), color ="red") +
ggtitle("MAE")
rmse_plot <- met_tab %>%
filter(.metric == "rmse") %>%
ggplot(aes(x = min_n, y = mtry, color = mean)) +
geom_point() +
geom_point(data = rmse_top, aes(x= min_n, y = mtry), color ="red") +
ggtitle("RMSE")
rsq_plot <- met_tab %>%
filter(.metric == "rsq") %>%
ggplot(aes(x = min_n, y = mtry, color = mean)) +
geom_point() +
geom_point(data = rsq_top, aes(x= min_n, y = mtry), color ="red") +
ggtitle("R-Squared")
grid.arrange(mae_plot, rmse_plot, rsq_plot, ncol = 1)
rf_reg <- rand_forest(trees = 100, mtry = tune(),min_n = tune()) %>%
set_mode("regression") %>%
set_engine("ranger")
rf_tune <- workflow() %>%
add_model(rf_reg) %>%
add_formula(body_mass_g ~ .)
peng_folds <- vfold_cv(peng_train)
tuned_rf <- tune_grid(rf_tune, resamples = peng_folds, grid = 36, metrics = metric_set(rmse, mae, rsq))
(met_tab <- tuned_rf %>%
#The collect metrics function often knows when to give us aggregated vs individual specs, based on the type of object we give it.
collect_metrics())
mae_top <- met_tab %>% filter(.metric == "mae") %>% slice_min(mean, n=1)
rmse_top <- met_tab %>% filter(.metric == "rmse") %>% slice_min(mean, n=1)
rsq_top <- met_tab %>% filter(.metric == "rsq") %>% slice_max(mean, n=1)
mae_plot <- met_tab %>%
filter(.metric == "mae") %>%
ggplot(aes(x = min_n, y = mtry, color = mean)) +
geom_point() +
geom_point(data = mae_top, aes(x= min_n, y = mtry), color ="red") +
ggtitle("MAE")
rmse_plot <- met_tab %>%
filter(.metric == "rmse") %>%
ggplot(aes(x = min_n, y = mtry, color = mean)) +
geom_point() +
geom_point(data = rmse_top, aes(x= min_n, y = mtry), color ="red") +
ggtitle("RMSE")
rsq_plot <- met_tab %>%
filter(.metric == "rsq") %>%
ggplot(aes(x = min_n, y = mtry, color = mean)) +
geom_point() +
geom_point(data = rsq_top, aes(x= min_n, y = mtry), color ="red") +
ggtitle("R-Squared")
grid.arrange(mae_plot, rmse_plot, rsq_plot, ncol = 1)
rf_tuned <- rand_forest(trees = 100, mtry = 4, min_n = 23) %>%
set_mode("regression") %>%
set_engine("ranger")
final_rf_wf <- workflow() %>%
add_model(rf_tuned) %>%
add_formula(body_mass_g ~ .)
peng_boots <- bootstraps(peng_train, times = 100)
#Training our tuned model on our bootstraps
(boot_mets <- final_rf_wf %>%
fit_resamples(peng_boots, metrics = metric_set(rmse, mae, rsq)))
#So now we have a pretty solid idea of how our model handles variations in data
boot_mets %>%
collect_metrics()
final_rf_wf %>%
last_fit(peng_split, metrics = metric_set(rmse, mae, rsq)) %>%
collect_metrics()
