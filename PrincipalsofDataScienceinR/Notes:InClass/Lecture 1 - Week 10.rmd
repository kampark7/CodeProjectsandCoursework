---
title: "Lecture 1 - Week 10 - Regression"
output: html_notebook
---

Returning to regression.
========

A brief review:
Linear regression - operates by fitting a line onto the data by minimizing some kind of loss function. This line is meant to illustrate a broader relationship trend between two or more variables.

The function behind a line - y = mx + b

m is a coefficient, b is the intercept. m generally says that, with each one-unit increase in x, y will increase by m.

Loss functions work by measuring residuals in the aggregate. R^2 (R-squared) measures the proportion of variance explained by the model.

Some loss statistics can be used to compare models applied to the same data (AIC, BIC, adjusted R-squared)

With some modification (polynomial degrees, or splines), linear regression models can be made less-linear, although this modification should not be taken lightly.

In some cases, you might also have interacting terms - the slope of your line might get steeper as a variable increases. Interaction terms can help with this as well, by capturing an additional relationship between two predictive variables.

Using Linear Models for Hypothesis Testing
========
```{r}
library(palmerpenguins)
library(tidyverse)
library(tidymodels)

penguins <- penguins %>%
  filter(complete.cases(bill_depth_mm))

model <- lm(bill_length_mm ~ bill_depth_mm, data = penguins)

summary(model)
```

```{r}
mean(penguins$bill_length_mm, na.rm=TRUE)

penguins %>%
  ggplot(aes(x = bill_depth_mm, y = bill_length_mm)) +
    xlim(0, 25) +
    geom_point() +
    geom_abline(slope = -0.6, intercept = 55.07, color="red") +
    geom_abline(slope = 0, intercept = 43.92, color="blue") +
  # A line using the population mean, with a slope of zero
    geom_text(x = 5, y= 45, label = "Null Hypothesis") +
  #  A trained regression line
    geom_text(x = 5, y = 53.3, label = "Alternative Hypothesis", angle = -18)
```
Comparing Our Hypotheses
========
Returning to the summary of our linear model.
```{r}
summary(lm(bill_length_mm ~ bill_depth_mm, data = penguins))
```

First, we calculate the sum of our squared residuals (SS) for our null hypothesis (a measure of the accuracy of the null hypothesis as a model). 
=======
```{r}
library(modelr)
penguins$null_preds <- 43.92
penguins <- add_predictions(penguins, model=model, var = "alt_preds")
penguins

penguins %>%
  ggplot(aes(x = bill_depth_mm, y = bill_length_mm)) +
    geom_point(alpha = 0.3) +
    geom_segment(aes(xend = bill_depth_mm, yend = null_preds), alpha = 0.33) +
    geom_line(aes(y = null_preds))
```

Second, we calculate the same thing for our linear regression model.
========
```{r}
penguins %>%
  ggplot(aes(x = bill_depth_mm, y = bill_length_mm)) +
    geom_point(alpha = 0.3) +
    geom_segment(aes(xend = bill_depth_mm, yend = alt_preds), alpha = 0.33) +
    geom_line(aes(y = alt_preds))
```

Third, we subtract the SS value for our null hypothesis from the SS value for our alternative hypothesis (to calculate how much better our model does at explaining variation compared to our null hypothesis).

Covering the Summary Again
=========

```{r}
summary(model)
```
R-squared equals the SS of our model (SSm) divided by the SS of our null hypothesis (SSt): SSm/SSt - the percent of variation in the data that our model explains, compared to how much variation there is under our null hypothesis.

Our F-statistic is calculated very similarly, but instead of the sum of squares, it uses the mean of squared residuals: MSm/MSr. In other words, average SSm/average SSr = F-Statistic. The F-statistic is a measure of the difference between the means of two populations (also used in ANOVA analysis) - an F-stat is essentially the measure of how much the model has improved over the null hypothesis.

The F-stat is used to calculate the overall p-value of the model (based on an F-distribution, but we've already talked about how that process works). The p-value in this case, is the odds that the model would improve our accuracy that much if our null-hypothesis was true.

Finally, there are t-values associated with each of our coefficients, along with p-values (Pr(>|t|)), which estimate the odds of each of those coefficients occuring.

Using Linear Models for Prediction
=======
Just like we can calculate a t-value for our coefficients, we can also calculate an expected distribution for each of our predictions. This requires an assumption of normality (or bootstrapping).
```{r}
library(gridExtra)
level <- 0.95

# A linear model confidence interval just means that 95% of sample values are expected to fall within this level - it's not actually good to use for predictions.
conf_levels <- cbind(penguins, predict(model, interval = "confidence", level = level))

# Prediction intervals are wider, because they account for individual variation.
# Note that building a prediction interval will give us a little warning
pred_levels <- cbind(penguins, predict(model, interval = "prediction", level = level))

confidence <- ggplot(conf_levels, aes(x=bill_depth_mm, y=bill_length_mm)) +
  geom_point() +
  geom_smooth(method = lm, level = 0.95, se = FALSE) +
  geom_line(aes(x = bill_depth_mm, y = lwr), color = "red") +
  geom_line(aes(x= bill_depth_mm, y = upr), color = "red")
  
prediction <- ggplot(pred_levels, aes(x=bill_depth_mm, y=bill_length_mm)) +
  geom_point() +
  geom_smooth(method = lm, level = 0.95, se = FALSE) +
  geom_line(aes(x = bill_depth_mm, y = lwr), color = "red") +
  geom_line(aes(x= bill_depth_mm, y = upr), color = "red")

grid.arrange(confidence, prediction)
```
Main takeaway re:prediction - if you want to use a linear model to predict *totally new* data, expect your prediction interval to be much larger than your confidence interval. If you are, however, trying to predict characteristics of additional samples from the same population (and you're sure it's really the same population), then you should be safe with a confidence interval.

Challenges of Linear Models
========
* Nonlinearity of data (with some flexibility)
* Correlations between error terms (non-random residuals)
    - Can occur with duplicated or time-series data (the residual for this day might be correlated with the residual for the next day, for example)
    - Can skew our standard error calculations - making it harder to construct accurate confidence intervals
    
* Non-constant variance of Error Terms, or *Heteroscedasticity* (Funnel-shaped residual plots, for example - shows that variance increases as some other variable increases)
   - Can mess with standard error calculations (the confidence interval should change according to the variables that influence variance, which can be tricky)
   
* Outliers
  - Linear models tend to be very sensitive to outliers - pulled in the direction of outliers - because they influence our loss functions more than other points.
  
* High leverage points
    - Similar to outliers, but they might fall outside of the bulk of the data along the *x* axis, instead of, or in addition to being unusual along the y axis.
    
* Collinearity - two or more predictor variables are closely related to each other
  - Makes it difficult to tell which predictor variable is really having an impact, and can inflate our standard error and p-values, to make it harder to tell which variables are really significant, if any are.
    - You can detect collinearity with a correlation matrix.
    

Creating a Correlation Matrix
========
```{r}
library(corrplot)

peng_nums <- penguins %>% select_if(is.numeric) %>% select(-c(null_preds, alt_preds))
peng_nums <- peng_nums %>% filter(complete.cases(.))
corr_chart <- round(cor(peng_nums), 1)
corr_chart

corrplot(corr_chart, method = "number")
```